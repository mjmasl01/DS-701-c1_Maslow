{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "- Clustering is an unsupervised machine learning technique used to group similar data points together based on their features.\n",
    "-  The primary goal of clustering is to discover the underlying structure or patterns within a dataset without prior knowledge of the group labels.\n",
    "\n",
    "### K-Means clustering is one of the most popular clustering algorithm used for analysis of unlabeled data.\n",
    "-  It aims to partition a dataset into k clusters, where each data point belongs to the cluster with the nearest mean (centroid).\n",
    "- The algorithm seeks to minimize the `Within-Cluster Sum of Squares (WCSS)`, which is the sum of the squared distances between each data point and its assigned cluster centroid.\n",
    "\n",
    "Advantages of this technique include:\n",
    "- Simplicity\n",
    "- Scalability\n",
    "- Speed (Based on initialization)\n",
    "\n",
    "Applications of K-Means\n",
    "- Customer Segmentation: Grouping customers based on purchasing behavior or demographics.\n",
    "- Image Segmentation: Dividing an image into regions for analysis or compression.\n",
    "- Document Clustering: Organizing documents into topics or themes.\n",
    "- Anomaly Detection: Identifying unusual data points that do not fit into any cluster.\n",
    "\n",
    "\n",
    "### In today's discussion, we'll be going through a visualization on K-Means clustering and how exactly it works under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:, 1], s=30)\n",
    "plt.title('Synthetic Blob Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we're using the random initialization\n",
    "print(X[0])\n",
    "centroids = X[np.random.choice(X.shape[0], 3, replace=False)]\n",
    "print(\"Initial centroids: \", centroids)\n",
    "cluster_history = []\n",
    "for _ in range(1):\n",
    "    #X[:, np.newaxis]: This adds a new axis to X, transforming it from a shape of (n_samples, n_features) to (n_samples, 1, n_features)\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)    #computes the euclidean distance along the feature axis. results in shape (n_samples, k)\n",
    "    print(\"Distance of data point 1 from the centroids: \", distances[0])\n",
    "    labels = np.argmin(distances, axis=1)                               #takes that index with smallest distance to a centroid\n",
    "    print(\"Closest cluster: \", labels[0])\n",
    "    cluster_history.append((centroids.copy(), labels.copy()))\n",
    "    new_centroids = np.array([X[labels==i].mean(axis=0) for i in range(3)])\n",
    "    print(\"Centroids after 1 iteration: \", new_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, np.newaxis] - centroids    #difference between each data point and each centroid, resulting in an array of shape (n_samples, k, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that performs k-means clustering\n",
    "def k_means(X, k, max_iters=10):\n",
    "    centroids = X[np.random.choice(X.shape[0], k, replace=False)]\n",
    "    cluster_history = []\n",
    "    for _ in range(max_iters):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)    #computes the euclidean distance along the feature axis. results in shape (n_samples, k)\n",
    "        labels = np.argmin(distances, axis=1)                               #takes that index with smallest distance to a centroid\n",
    "        cluster_history.append((centroids.copy(), labels.copy()))\n",
    "        new_centroids = np.array([X[labels==i].mean(axis=0) for i in range(k)])\n",
    "        if np.all(centroids == new_centroids):  # Check for convergence\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return labels, centroids, cluster_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=1, random_state=42)\n",
    "k = 4\n",
    "labels, final_centroids, history = k_means(X, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = len(history)\n",
    "fig, axs = plt.subplots(1, num_iterations, figsize=(15, 5))\n",
    "for i, (centroids, labels) in enumerate(history):\n",
    "    axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30)\n",
    "    axs[i].scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')\n",
    "    axs[i].set_title(f'Iteration {i + 1}')\n",
    "    axs[i].set_xlabel('Feature 1')\n",
    "    axs[i].set_ylabel('Feature 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means ++\n",
    "\n",
    "- k-Means++ is an enhancement of the traditional k-means clustering algorithm that improves the selection of initial centroids\n",
    "- This method aims to address the sensitivity of k-means to the initial placement of centroids, which can affect the `quality and convergence speed` of the clustering process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(X, k):\n",
    "    n_samples = X.shape[0]\n",
    "    centroids = np.empty((k, X.shape[1]))\n",
    "\n",
    "    # Randomly choose the first centroid\n",
    "    centroids[0] = X[np.random.choice(n_samples)]\n",
    "\n",
    "    for i in range(1, k):\n",
    "        # Compute distances from each point to the nearest centroid\n",
    "        distances = np.min(np.linalg.norm(X[:, np.newaxis] - centroids[:i], axis=2), axis=1)\n",
    "        # Choose the next centroid with a probability proportional to the square of the distance\n",
    "        probabilities = distances**2 / np.sum(distances**2)\n",
    "        cumulative_probabilities = np.cumsum(probabilities)\n",
    "        r = np.random.rand()\n",
    "        for j, p in enumerate(cumulative_probabilities):\n",
    "            if r < p:\n",
    "                centroids[i] = X[j]\n",
    "                break\n",
    "                \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that performs k-means clustering\n",
    "def k_means_pp(X, k, max_iters=10):\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    cluster_history = []\n",
    "    for _ in range(max_iters):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)    #computes the euclidean distance along the feature axis. results in shape (n_samples, k)\n",
    "        labels = np.argmin(distances, axis=1)                               #takes that index with smallest distance to a centroid\n",
    "        cluster_history.append((centroids.copy(), labels.copy()))\n",
    "        new_centroids = np.array([X[labels==i].mean(axis=0) for i in range(k)])\n",
    "        if np.all(centroids == new_centroids):  # Check for convergence\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return labels, centroids, cluster_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "labels_kpp, final_centroids_kpp, history_kpp = k_means(X, k)\n",
    "num_iterations = len(history_kpp)\n",
    "fig, axs = plt.subplots(1, num_iterations, figsize=(15, 5))\n",
    "for i, (centroids, labels) in enumerate(history_kpp):\n",
    "    axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30)\n",
    "    axs[i].scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')\n",
    "    axs[i].set_title(f'Iteration {i + 1}')\n",
    "    axs[i].set_xlabel('Feature 1')\n",
    "    axs[i].set_ylabel('Feature 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the k-means clustering model\n",
    "\n",
    "#### Silhoutte Score\n",
    "- This is one of the most popularly used metric to evaluate the clustering by the K-Means algorithm on a given dataset.\n",
    "- The silhouette score measures how `similar` an object is to its `own cluster compared to other clusters`\n",
    "- It ranges from `-1 to 1`, where a higher score indicates better-defined clusters\n",
    "\n",
    "##### For each sample, compute the `mean intra-cluster distance` (a) and the `mean nearest-cluster distance` (b). The silhouette score for a sample is given by:\n",
    "#### Silhouette Score = (b-a)/max(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, random_state=10)\n",
    "\n",
    "# Apply k-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, init='random')\n",
    "labels = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Calculate silhouette score\n",
    "score = silhouette_score(X, labels)\n",
    "print(f'Silhouette Score: {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=100)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')\n",
    "plt.title('Cluster Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
